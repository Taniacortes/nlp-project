{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eba1e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando todas las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "\n",
    "# Bibliotecas para el pre procesamiento del texto\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Bibliotecas para el uso de GloVe\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para la construccion de modelos\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1738456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10053]\n",
      "[nltk_data]     Se ha anulado una conexión establecida por el software\n",
      "[nltk_data]     en su equipo host>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10053] Se\n",
      "[nltk_data]     ha anulado una conexión establecida por el software en\n",
      "[nltk_data]     su equipo host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se descargan las stopwords y lemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05984cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_texto(textos):\n",
    "    '''\n",
    "        Esta funcion permite realizar el pre procesamiento a los textos, en donde primero se eliminan \n",
    "        caracteres especiales y ajenos al alfabeto en inglés. Posteriormente se separa el texto en tokens, \n",
    "        se les aplica un proceso de lematización, se eliminan las stop words y se regresa el texto a su estado original.\n",
    "    '''\n",
    "    # Se inicializa el conjunto de stopwords y el lemmatizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Se tiene una lista en donde se almacenaran los textos despues del pre procesamiento\n",
    "    textos_preprocesados = []\n",
    "    \n",
    "    for texto in textos:\n",
    "        # Utilizando expresiones regulares se eliminan signos de puntuacion y cualquier otro caracter ajeno al ingles\n",
    "        # Asimismo se pasa el texto a minusculas\n",
    "        texto = re.sub(r'[^a-z\\s]', '', texto.lower())\n",
    "\n",
    "        # Se divide el texto en palabras (tokens)\n",
    "        tokens = texto.split()\n",
    "        \n",
    "        # Se eliminan las stop words y se aplica el lemmatizer\n",
    "        palabras_filtradas = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Se vuelven a unir las palabras para conformar un mismo texto ya preprocesado\n",
    "        texto_preprocesado = ' '.join(palabras_filtradas)\n",
    "        \n",
    "        # Se une el texto preprocesado a la lista de textos\n",
    "        textos_preprocesados.append(texto_preprocesado)\n",
    "    \n",
    "    # Se devuelven todos los textos ya pre procesados\n",
    "    return textos_preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2279a09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se carga el archivo a leer\n",
    "df = pd.read_csv('Suicide_Detection.csv')\n",
    "\n",
    "# Se extraen las etiquetas y textos\n",
    "textos = df['text'].values\n",
    "etiquetas = df['class'].values\n",
    "\n",
    "# Se realiza el preprocesamiento de textos\n",
    "textos_preprocesados = preprocesar_texto(textos)\n",
    "\n",
    "# Se separa el conjunto de datos en un conjunto de entrenamiento y otro de prueba\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(textos_preprocesados, etiquetas, test_size=0.2, random_state=42)\n",
    "\n",
    "# El siguiente apartado aplica solo para los modelos de Naive Bayes y Regresion Logistica\n",
    "# Se obtienen las caracteristicas de los textos usando el modelo de bolsa de palabras\n",
    "vectorizer = CountVectorizer()\n",
    "features_train = vectorizer.fit_transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "\n",
    "# Se escalan las caracteristicas anteriores para tener los valores dentro de un intervalo de valores mas proximo\n",
    "# Se hace uso de el metodo de estandarizacion\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "features_train_scaled = scaler.fit_transform(features_train)\n",
    "features_test_scaled = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1b07c",
   "metadata": {},
   "source": [
    "# Modelo con Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f21276a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9018636216740278\n"
     ]
    }
   ],
   "source": [
    "# Se entrena el modelo utilizando el algoritmo de Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "# Se utiliza el modelo sobre los datos y etiquetas de entrenamiento\n",
    "naive_bayes.fit(features_train, labels_train)\n",
    "\n",
    "# Se hacen predicciones con base en el conjunto de datos de prueba\n",
    "predictions_naive_bayes = naive_bayes.predict(features_test)\n",
    "\n",
    "# Se calcula la precision del modelo\n",
    "accuracy = accuracy_score(labels_test, predictions_naive_bayes)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eeff36",
   "metadata": {},
   "source": [
    "# Modelo con Regresion Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69133e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.933900678659916\n"
     ]
    }
   ],
   "source": [
    "# Se entrena el modelo utilizando el algoritmo de Regresion Logistica\n",
    "logistic = LogisticRegression(max_iter=1000, C=0.5)\n",
    "# Se utiliza el modelo sobre los datos y etiquetas de entrenamiento\n",
    "logistic.fit(features_train, labels_train)\n",
    "\n",
    "# Se hacen predicciones con base en el conjunto de datos de prueba\n",
    "predictions_logistic_regresion = logistic.predict(features_test)\n",
    "\n",
    "# Se calcula la precision del modelo\n",
    "accuracy = accuracy_score(labels_test, predictions_logistic_regresion)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2ce94",
   "metadata": {},
   "source": [
    "# Modelo con LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89434c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\AppData\\Local\\Temp\\ipykernel_6780\\1287770451.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Este apartado es para convertir el archivo que contiene GloVe previamente procesados a un formato entendible\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.50d.txt'\n",
    "word2vec_output_file = 'glove.6B.50d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74930461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_GloVe(textos, padding=50, glove_file='glove.6B.50d.word2vec.txt'):\n",
    "    '''\n",
    "        Esta funcion permite obtener los Global Vectors (GloVe) para poder realizar la obtencion de caracteristicas\n",
    "    '''\n",
    "    # Se usa un conjunto previamente entrenado para GloVe con 50 dimensiones\n",
    "    glove_model = KeyedVectors.load_word2vec_format('glove.6B.50d.word2vec.txt', binary=False)\n",
    "    \n",
    "    # Se tiene una lista en donde se van a almcenar las secuencias de vectores\n",
    "    sequences = []\n",
    "    \n",
    "    # Se obtendran los respectivos vectores para cada uno de los textos\n",
    "    for texto in textos:\n",
    "        # Se obtiene el valor del vector de aquellas palabras que se encuentren en el archivo previamente entrenado\n",
    "        sequence = [glove_model.key_to_index[word] for word in texto if word in glove_model.key_to_index]\n",
    "        # Se adiciona el vector a la lista de vectores\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    # Se busca que todos los vectores tengan el mismo tamaño\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=padding)\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f81e324c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2901/2901 [==============================] - 691s 238ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.5017\n",
      "Epoch 2/10\n",
      "2901/2901 [==============================] - 684s 236ms/step - loss: 0.6932 - accuracy: 0.4983 - val_loss: 0.6931 - val_accuracy: 0.5017\n",
      "Epoch 3/10\n",
      "2901/2901 [==============================] - 688s 237ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.4983\n",
      "Epoch 4/10\n",
      "2901/2901 [==============================] - 695s 240ms/step - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6931 - val_accuracy: 0.5017\n",
      "Epoch 5/10\n",
      "2901/2901 [==============================] - 703s 242ms/step - loss: 0.6932 - accuracy: 0.4991 - val_loss: 0.6932 - val_accuracy: 0.4983\n",
      "Epoch 6/10\n",
      "2901/2901 [==============================] - 710s 245ms/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.5017\n",
      "Epoch 7/10\n",
      "2901/2901 [==============================] - 713s 246ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4983\n",
      "Epoch 8/10\n",
      "2901/2901 [==============================] - 720s 248ms/step - loss: 0.6932 - accuracy: 0.4981 - val_loss: 0.6932 - val_accuracy: 0.4983\n",
      "Epoch 9/10\n",
      "2901/2901 [==============================] - 711s 245ms/step - loss: 0.6932 - accuracy: 0.4984 - val_loss: 0.6931 - val_accuracy: 0.4983\n",
      "Epoch 10/10\n",
      "2901/2901 [==============================] - 718s 247ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6933 - val_accuracy: 0.4983\n",
      "1451/1451 [==============================] - 10s 7ms/step - loss: 0.6933 - accuracy: 0.4983\n",
      "Accuracy: 0.4982872009277344\n"
     ]
    }
   ],
   "source": [
    "# Se separa el texto en tokens para poder aplicar GloVe\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(textos_preprocesados)\n",
    "sequences = tokenizer.texts_to_sequences(textos_preprocesados)\n",
    "\n",
    "# Se divide el conjunto de datos para entrenamiento y pruebas\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "    sequences, etiquetas, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Obteniendo los GloVe\n",
    "glove_train = obtener_GloVe(texts_train)\n",
    "glove_test = obtener_GloVe(texts_test)\n",
    "\n",
    "# Se convierten las etiquetas a valores numericos para poder utilizar el modelo\n",
    "labelEnc=LabelEncoder()\n",
    "labels_trainEnc=labelEnc.fit_transform(labels_train)\n",
    "labels_testEnc=labelEnc.transform(labels_test)\n",
    "\n",
    "# Se define el modelo LSTM con tres capas: Embedding, LSTM y Dense con funcion de activacion sigmoide\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=50))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# El modelo se compila utilizando una funcion de entropia cruzada y el optimizador con el algoritmo de Adam\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Se entrena el modelo\n",
    "model.fit(glove_train, labels_trainEnc, validation_data=(glove_test, labels_testEnc), epochs=10, batch_size=64)\n",
    "\n",
    "# Se evalua el modelo\n",
    "loss, accuracy = model.evaluate(glove_test, labels_testEnc)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0699c",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ae4a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean algunos textos de ejemplos para ver qué tal reacciona cada modelo\n",
    "# Asimismo se pone la intención que tiene cada texto\n",
    "texto_ejemplo1 = \"I have been feeling pretty lonely lately. I don't know how to solve all of my problems, everything seems harder every day that passes. I think the only good solution for me is to end with all my suffering and end my life for good.\" # Intencion suicida\n",
    "texto_ejemplo2 = \"I have been feeling pretty lonely lately, as I broke up with my girlfriend recently and I don't know how to cope with it. Despite the fact that i want all the suffering to go away, I know that all of this is a process and someday i will get over this.\" # Sin intencion suicida\n",
    "texto_ejemplo3 = \"There are a lot of things happening in my head recently. My mom just passed away and I am having problems with my partner. I need some advices and help to go through all of this.\" # Sin intencion suicida\n",
    "texto_ejemplo4 = \"There are a lot of things happening in my head recently. My mom just passed away and I am having problems with my partner. I am having a lot of suicidal thoughts and i can't stop thinking it is the only solution.\" # Intencion suicida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e0a8acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Una vez creados los textos se necesitan preprocesar para obtener mejores resultados\n",
    "secuencia1 = preprocesar_texto([texto_ejemplo1])\n",
    "secuencia2 = preprocesar_texto([texto_ejemplo2])\n",
    "secuencia3 = preprocesar_texto([texto_ejemplo3])\n",
    "secuencia4 = preprocesar_texto([texto_ejemplo4])\n",
    "\n",
    "# Posteriormente a que fueron preprocesados se les aplica la extraccion de caracteristicas\n",
    "vector_texto1 = vectorizer.transform(secuencia1)\n",
    "vector_texto2 = vectorizer.transform(secuencia2)\n",
    "vector_texto3 = vectorizer.transform(secuencia3)\n",
    "vector_texto4 = vectorizer.transform(secuencia4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdfc0a",
   "metadata": {},
   "source": [
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d97b33f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tResultados:\n",
      "-->Para el primer texto se predijo la etiqueta ['suicide']\n",
      "-->Para el segundo texto se predijo la etiqueta ['suicide']\n",
      "-->Para el tercer texto se predijo la etiqueta ['suicide']\n",
      "-->Para el cuarto texto se predijo la etiqueta ['suicide']\n"
     ]
    }
   ],
   "source": [
    "# Se realizan las predicciones de los cuatro textos\n",
    "resultado1 = naive_bayes.predict(vector_texto1)\n",
    "resultado2 = naive_bayes.predict(vector_texto2)\n",
    "resultado3 = naive_bayes.predict(vector_texto3)\n",
    "resultado4 = naive_bayes.predict(vector_texto4)\n",
    "print(\"\\tResultados:\")\n",
    "print(f'-->Para el primer texto se predijo la etiqueta {resultado1}')\n",
    "print(f'-->Para el segundo texto se predijo la etiqueta {resultado2}')\n",
    "print(f'-->Para el tercer texto se predijo la etiqueta {resultado3}')\n",
    "print(f'-->Para el cuarto texto se predijo la etiqueta {resultado4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03deddf9",
   "metadata": {},
   "source": [
    "* Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5fe66751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tResultados:\n",
      "-->Para el primer texto se predijo la etiqueta ['suicide']\n",
      "-->Para el segundo texto se predijo la etiqueta ['non-suicide']\n",
      "-->Para el tercer texto se predijo la etiqueta ['non-suicide']\n",
      "-->Para el cuarto texto se predijo la etiqueta ['suicide']\n"
     ]
    }
   ],
   "source": [
    "# Se realizan las predicciones de los cuatro textos\n",
    "resultado1 = logistic.predict(vector_texto1)\n",
    "resultado2 = logistic.predict(vector_texto2)\n",
    "resultado3 = logistic.predict(vector_texto3)\n",
    "resultado4 = logistic.predict(vector_texto4)\n",
    "print(\"\\tResultados:\")\n",
    "print(f'-->Para el primer texto se predijo la etiqueta {resultado1}')\n",
    "print(f'-->Para el segundo texto se predijo la etiqueta {resultado2}')\n",
    "print(f'-->Para el tercer texto se predijo la etiqueta {resultado3}')\n",
    "print(f'-->Para el cuarto texto se predijo la etiqueta {resultado4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d78e7",
   "metadata": {},
   "source": [
    "* LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10782aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posteriormente a que fueron preprocesados se les aplica la extraccion de caracteristicas con GloVe \n",
    "texto_tokens = tokenizer.texts_to_sequences([secuencia1, secuencia2, secuencia3, secuencia4])\n",
    "glove_textos = obtener_GloVe(texto_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20ea9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 450ms/step\n"
     ]
    }
   ],
   "source": [
    "resultados = model.predict(glove_textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f57b6b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(vector_texto4.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc95db3",
   "metadata": {},
   "source": [
    "> En este caso los valores mayores a 0.5 se toman como un 1, lo cual significa que la etiqueta predicha es 'suicide'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ad26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
